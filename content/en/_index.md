---
title: "Trustworthy Collaborative AI"
date: 2023-05-29T10:10:20+08:00
tags: []
featured_image: "/images/background.jpg"
description: "NeurIPS 2023 Workshop"

---

# OVERVIEW

Collaborative AI systems require multiple agents, including machines and humans, to work together as partners to achieve a common goal, sharing a mutual understanding of the abilities and respective roles of each other. There are a large number of applications where collaborative AI systems can be applied, such as intelligent driver assistance systems, AIoT for medicine, and robots for health and social care. However, many current AI systems are found vulnerable to imperceptible attacks, biased against underrepresented groups, and lacking user privacy protection and explainability. These shortcomings degrade the user experience and erode people’s trust in all AI systems. Therefore, it is imperative to identify the need for a paradigm shift toward comprehensively trustworthy collaborative AI systems. Specifically, it is crucial to consider the potential risks associated with decision-critical domains using AI, such as privacy, bias, security, lack of explainability, ethics, regulation, etc.

This workshop titled “Trustworthy Collaborative AI Systems in Real-World Applications: Pitfalls and Opportunities” aims to address critical challenges and opportunities in trustworthiness and collaborative systems for AI. We aim to provide a platform for experts and researchers to explore novel techniques, share best practices, and discuss ethical considerations to develop AI systems that are reliable and interpretable and capable of seamless collaboration within multi-intelligent-entity environments. By fostering a deeper understanding of how to ensure trustworthiness and effective collaboration in these systems, we can advance the development and deployment of AI in real-world applications, leading to safer, more reliable, and socially beneficial intelligent systems.

The following is a non-exhaustive list of topics we aim to address through our invited talks, panels, and accepted papers:

* The current state and progression of collaborative learning systems, particularly in the context of multi-agent systems, socially capable robots, and human-robot interaction across various real-world applications.

* Strategies for enhancing trustworthy aspects (e.g., interpretability, privacy preservation, fair- ness, security, robustness, and generalizability) in collaborative learning systems, including federated learning, split learning, multi-agent learning, and multi-domain learning.

* Existential risk reduction and governance in real-world applications (e.g., healthcare) using collaborative AI systems.

* Trustworthy multi-agent learning and applications, such as multi-agent robotics and collabo- rative autonomous systems.

* Trustworthy collaborative learning over IoT devices and applications.

* Theoretical foundations for trustworthy collaborative learning systems, aiming to provide a solid base for future developments.

* New benchmark datasets and evaluation metrics.


**Program Committee**

* Yingfan Wang (Duke University, USA)   
* Ziyan Wang	(Georgia Institute of Technology, USA)   
* Zihao Xu	(Rutgers University, USA)  
* Xinyu Xu	(The Hong Kong University of Science and Technology, Hong Kong, China)  
* Miao Li	(Wuhan University, China)  
* Sylvain Calinon	(Idiap Research Institute, Switzerland)  
* Fanny Ficuciello	(University of Naples Federico II, Italy)  
* Qianyi Chen	(The Hong Kong Polytechnic University, China)  
* Tianhang Zheng	(University of Toronto, Canada)  
* Peifei Zhu	(Tianjin University, China)  
* Aoxiao Zhong (Harvard University, USA)
* Guo Zhang (Massachusetts Institute of Technology, USA)  
* Ziyi Zhang  (South China University of Technology, China)  
* Mingxuan Ouyang  (The Hong Kong Polytechnic University, Hong Kong, China)         
